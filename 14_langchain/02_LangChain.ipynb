{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOycdBmh0FBf"
      },
      "source": [
        "<h1 align=\"left\"><b><font color=\"yellow\" size=\"50\">\n",
        "LangChain hands on Workshop for making AI apps and software like chatGPT\n",
        "</font></b></h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTDgRy0jKDkP"
      },
      "source": [
        "# LangChain\n",
        "\n",
        "LangChain is a framework for developing applications powered by language models.\n",
        "\n",
        "- GitHub: https://github.com/hwchase17/langchain\n",
        "- Docs: https://python.langchain.com/en/latest/index.html\n",
        "\n",
        "### Overview:\n",
        "- Installation\n",
        "- LLMs\n",
        "- Prompt Templates\n",
        "- Chains\n",
        "- Agents and Tools\n",
        "- Memory\n",
        "- Document Loaders\n",
        "- Indexes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WGtOYYTKfz3"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bcrn7QRyQXGj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (0.0.238)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from langchain) (2.0.19)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from langchain) (0.5.13)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.11 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from langchain) (0.0.12)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from langchain) (1.2.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from langchain) (1.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from langchain) (2.28.2)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from requests<3,>=2->langchain) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
            "Requirement already satisfied: packaging>=17.0 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkGGSdmtta6s"
      },
      "source": [
        "## 1. LLMs\n",
        "\n",
        "A generic interface for all LLMs. See all LLM providers: https://python.langchain.com/en/latest/modules/models/llms/integrations.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "H_dfy6G_aBtY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (0.27.7)\n",
            "Requirement already satisfied: requests>=2.20 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from openai) (2.28.2)\n",
            "Requirement already satisfied: tqdm in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from openai) (4.64.1)\n",
            "Requirement already satisfied: aiohttp in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from requests>=2.20->openai) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from requests>=2.20->openai) (2023.5.7)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from aiohttp->openai) (22.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from aiohttp->openai) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RlxEmS1CaM5v"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] =\"write your api key here\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to make get an API key for openAI?\n",
        "Sign up for API key on huggingface and go to https://platform.openai.com/account/api-keys and copy your API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pY09s9cmZ6nQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Python Basics Urdu/Hindi\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import OpenAI\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] =\"write your api key here\"\n",
        "llm = OpenAI(temperature=0.9)  # model_name=\"text-davinci-003\"\n",
        "text = \"What would be a nice name for Youtube channel about Python for beginners in urdu/hindi?\"\n",
        "print(llm(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "idkq_aVyaceF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: huggingface_hub in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (0.16.4)\n",
            "Requirement already satisfied: filelock in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from huggingface_hub) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from huggingface_hub) (2.28.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from huggingface_hub) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from huggingface_hub) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from huggingface_hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from huggingface_hub) (23.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from requests->huggingface_hub) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from requests->huggingface_hub) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from requests->huggingface_hub) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/babaaammar/mambaforge/envs/python_ml/lib/python3.11/site-packages (from requests->huggingface_hub) (2023.5.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "i4DKOWjyaRmO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to make get an API key for HuggingFace?\n",
        "Sign up for API key on huggingface and go to https://huggingface.co/settings/tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "QmtH72oCaU32"
      },
      "outputs": [],
      "source": [
        "from langchain import HuggingFaceHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "8uK5TtJPc49I"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Wie alt bist du?'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Use a smaller model\n",
        "llm = HuggingFaceHub(repo_id=\"t5-base\", model_kwargs={\"temperature\":0, \"max_length\":64})\n",
        "llm(\"translate English to German: How old are you?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O-7dO1htdO4"
      },
      "source": [
        "## 2. Prompt Templates\n",
        "\n",
        "LangChain faciliates prompt management and optimization.\n",
        "\n",
        "Normally when you use an LLM in an application, you are not sending user input directly to the LLM. Instead, you need to take the user input and construct a prompt, and only then send that to the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_FDS9IDRapOt"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Kann Barack Obama ein Gespräch mit George Washington führen?'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm(\"Can Barack Obama have a conversation with George Washington?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "lB4W8dM1tPAY"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Frage: Kann Barack Obama ein Gespräch mit George Washington führen?'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"\"\"Question: Can Barack Obama have a conversation with George Washington?\n",
        "\n",
        "Let's think step by step.\n",
        "\n",
        "Answer: \"\"\"\n",
        "llm(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "UU1VyMMvtsCE"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Let's think step by step.\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "-Yzpc_0aHHeE"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Question: Can Barack Obama have a conversation with George Washington?\\n\\nLet's think step by step.\\n\\nAnswer: \""
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt.format(question=\"Can Barack Obama have a conversation with George Washington?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "on8ubh3kt7oD"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Argument `prompt` is expected to be a string. Instead found <class 'langchain_core.prompts.prompt.PromptTemplate'>. If you want to run the LLM on multiple prompts, use `generate` instead.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/mambaforge/envs/streamlit_env/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     emit_warning()\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/mambaforge/envs/streamlit_env/lib/python3.9/site-packages/langchain_core/language_models/llms.py:985\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check Cache and run the LLM on the given prompt and input.\"\"\"\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 985\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    987\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`generate` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    989\u001b[0m     )\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    992\u001b[0m         [prompt],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1000\u001b[0m     \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m   1001\u001b[0m )\n",
            "\u001b[0;31mValueError\u001b[0m: Argument `prompt` is expected to be a string. Instead found <class 'langchain_core.prompts.prompt.PromptTemplate'>. If you want to run the LLM on multiple prompts, use `generate` instead."
          ]
        }
      ],
      "source": [
        "llm(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zw1KlSeuUOY"
      },
      "source": [
        "## 3. Chains\n",
        "\n",
        "Combine LLMs and Prompts in multi-step workflows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eE6n-jbAuOxt"
      },
      "outputs": [],
      "source": [
        "from langchain import LLMChain\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "question = \"Can Barack Obama have a conversation with George Washington?\"\n",
        "\n",
        "print(llm_chain.run(question))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp-UlOK0bMVQ"
      },
      "source": [
        "## 4. Agents and Tools\n",
        "\n",
        "Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done.\n",
        "\n",
        "\n",
        "When used correctly agents can be extremely powerful. In order to load agents, you should understand the following concepts:\n",
        "\n",
        "- Tool: A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains.\n",
        "- LLM: The language model powering the agent.\n",
        "- Agent: The agent to use.\n",
        "\n",
        "Tools: https://python.langchain.com/en/latest/modules/agents/tools.html\n",
        "\n",
        "Agent Types: https://python.langchain.com/en/latest/modules/agents/agents/agent_types.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79JcjhFXwv0J"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOSpaurEb1MR"
      },
      "outputs": [],
      "source": [
        "!pip install wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgV4kny1bgy1"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI(temperature=0)\n",
        "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQUOsWLrbjKv"
      },
      "outputs": [],
      "source": [
        "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8Rob2Wsb_l9"
      },
      "outputs": [],
      "source": [
        "agent.run(\"In what year was the film Departed with Leopnardo Dicaprio released? What is this year raised to the 0.43 power?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AuQNfhYm48A"
      },
      "source": [
        "## 5. Memory\n",
        "\n",
        "Add State to Chains and Agents.\n",
        "\n",
        "Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ujwj29G2cDPN"
      },
      "outputs": [],
      "source": [
        "from langchain import OpenAI, ConversationChain\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "conversation = ConversationChain(llm=llm, verbose=True)\n",
        "\n",
        "conversation.predict(input=\"Hi there!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkKv8n7ZnB2e"
      },
      "outputs": [],
      "source": [
        "conversation.predict(input=\"Can we talk about AI?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4P3zWCmoDST"
      },
      "outputs": [],
      "source": [
        "conversation.predict(input=\"I'm interested in Reinforcement Learning.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wMttXM-CuPK"
      },
      "source": [
        "## 6. Document Loaders\n",
        "\n",
        "Combining language models with your own text data is a powerful way to differentiate them. The first step in doing this is to load the data into “documents” - a fancy way of say some pieces of text. This module is aimed at making this easy.\n",
        "\n",
        "https://python.langchain.com/en/latest/modules/indexes/document_loaders.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAiISOcboPKR"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import NotionDirectoryLoader\n",
        "\n",
        "loader = NotionDirectoryLoader(\"Notion_DB\")\n",
        "\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_zcj8MLDGfQ"
      },
      "source": [
        "## 7. Indexes\n",
        "\n",
        "Indexes refer to ways to structure documents so that LLMs can best interact with them. This module contains utility functions for working with documents\n",
        "\n",
        "- Embeddings: An embedding is a numerical representation of a piece of information, for example, text, documents, images, audio, etc.\n",
        "- Text Splitters: When you want to deal with long pieces of text, it is necessary to split up that text into chunks.\n",
        "- Vectorstores: Vector databases store and index vector embeddings from NLP models to understand the meaning and context of strings of text, sentences, and whole documents for more accurate and relevant search results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLU79cyCozYl"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/hwchase17/langchain/master/docs/modules/state_of_the_union.txt\"\n",
        "res = requests.get(url)\n",
        "with open(\"state_of_the_union.txt\", \"w\") as f:\n",
        "  f.write(res.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGyZXiJZBsov"
      },
      "outputs": [],
      "source": [
        "# Document Loader\n",
        "from langchain.document_loaders import TextLoader\n",
        "loader = TextLoader('./state_of_the_union.txt')\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OklI0xTvp2KE"
      },
      "outputs": [],
      "source": [
        "# Text Splitter\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skvXSMXHCxyq"
      },
      "outputs": [],
      "source": [
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1yCdAhSCi64"
      },
      "outputs": [],
      "source": [
        "# Embeddings\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "\n",
        "#text = \"This is a test document.\"\n",
        "#query_result = embeddings.embed_query(text)\n",
        "#doc_result = embeddings.embed_documents([text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8R3pT55b-uBJ"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7sRydnlC7rb"
      },
      "outputs": [],
      "source": [
        "# Vectorstore: https://python.langchain.com/en/latest/modules/indexes/vectorstores.html\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "db = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "query = \"What did the president say about Ketanji Brown Jackson\"\n",
        "docs = db.similarity_search(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CB7lvDWzDHZy",
        "outputId": "3b0399d0-6c04-4cef-a029-e48cbd41eedd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \n",
            "\n",
            "Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n",
            "\n",
            "One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n",
            "\n",
            "And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\n"
          ]
        }
      ],
      "source": [
        "print(docs[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nu-AmhDLEK0h"
      },
      "outputs": [],
      "source": [
        "db.save_local(\"faiss_index\")\n",
        "new_db = FAISS.load_local(\"faiss_index\", embeddings)\n",
        "docs = new_db.similarity_search(query)\n",
        "print(docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1lGH_g2--Si"
      },
      "source": [
        "## End-to-end example\n",
        "\n",
        "https://github.com/hwchase17/chat-langchain\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
